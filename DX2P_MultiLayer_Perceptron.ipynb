{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q0eIaqT05tYW"
   },
   "source": [
    "# **Solving the classification of Iris Dataset : Single Layer Perceptron from Scratch**\n",
    "\n",
    "1.   List item\n",
    "2.   List item\n",
    "**bold text**\n",
    "\n",
    "\n",
    "Task: To predict the class of the iris flower according to the sepal length, sepal width, petal length and petal width\n",
    "\n",
    "**What is Iris dataset?**\n",
    "\n",
    "It is a dataset about classifying three different classes (Setosa, Versicolour, Virginica) of iris flower on the basis of four features such as sepal length, sepal width, petal length and petal width.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yde9-2RIGXug"
   },
   "source": [
    "\n",
    "In our code, we have 3 classes numerically defined as Setosa : 1, Versicolour : 2, Virginica : 3. We have four 2-featured instances as our input. We have 50 rows of each class, i.e. 150 data rows. We need to predict the class of the iris flower by taking a single layer perceptron with four features.\n",
    "\n",
    "**What does coding a neural network from scratch imply?**\n",
    "\n",
    "We do not use inbuilt libraries for machine/ deep learning such as those in scikit-learn, keras or tensor flow.  We use only numpy, pandas, and write functions to make the model, train, fit, predict and evaluate it. \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avbYReabDIFO"
   },
   "source": [
    "## STEP 1. Import the necessary libraries/ functions and get the data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOF3RGEoWI9b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1qext1qt8Nrf"
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tPzVlj1v2i8d",
    "outputId": "e83282d6-9b43-48a8-d1b0-a2701c6608c5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sg2lferiA-Vw"
   },
   "outputs": [],
   "source": [
    "num_classes=len(np.unique(y))\n",
    "a=np.zeros((y.shape[0], num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QcPGHkDvDvpM"
   },
   "outputs": [],
   "source": [
    "for i in range(y.shape[0]):\n",
    "  if y[i] == 0:\n",
    "    a[i][0] = 1\n",
    "  elif y[i] == 1:\n",
    "    a[i][1] = 1\n",
    "  else:\n",
    "    a[i][2] = 1\n",
    "\n",
    "#one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7cG2REGu3K_K"
   },
   "outputs": [],
   "source": [
    "X= pd.DataFrame(X,columns=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"])\n",
    "y = pd.DataFrame(a, columns = [\"y0\",\"y1\",\"y2\"],dtype=\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "rJJUsl1x3mLC",
    "outputId": "a742a659-88b5-485f-a31f-ac44bfda01df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y0</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y0  y1  y2\n",
       "0   1   0   0\n",
       "1   1   0   0\n",
       "2   1   0   0\n",
       "3   1   0   0\n",
       "4   1   0   0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "r1SqNnfKNbwx",
    "outputId": "3f2c97dd-0226-412c-ab65-19c3f0e139f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=np.array([[1,2,3]])\n",
    "np.argmax(b,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "CvmyX80WDWFA",
    "outputId": "cbcd2326-0927-40cb-a7fb-517dd14bead4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width\n",
       "0             5.1          3.5           1.4          0.2\n",
       "1             4.9          3.0           1.4          0.2\n",
       "2             4.7          3.2           1.3          0.2\n",
       "3             4.6          3.1           1.5          0.2\n",
       "4             5.0          3.6           1.4          0.2\n",
       "..            ...          ...           ...          ...\n",
       "145           6.7          3.0           5.2          2.3\n",
       "146           6.3          2.5           5.0          1.9\n",
       "147           6.5          3.0           5.2          2.0\n",
       "148           6.2          3.4           5.4          2.3\n",
       "149           5.9          3.0           5.1          1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In our code, we want to integrate the bias terms into the matrix of input value. \n",
    "#This way, our weighted sum becomes the following: WX instead of WX + b, where b #is a separate, bias vector\n",
    "\n",
    "#Inserting a new column into the pandas dataframe where every elemnt is 1, in the first position\n",
    "#X.insert(0,\"bias\", np.ones(X.shape[0]), True)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VfZLxRaDDWoZ"
   },
   "source": [
    "## STEP 2: Write the feed forward function that predicts the outcome using the weights and inputs\n",
    "\n",
    "This function is used to predict the activated output at a layer, and is called for each instance of X (corresponds to each row in the X matrix). It thus takes in a row of the input features (= the vector of feature values for that instance) and the updated weights as its parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uiScivihN_9t"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGzSWlDxWifW"
   },
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\" \n",
    "\n",
    "    e_x = np.exp(x - np.max(x)) \n",
    "\n",
    "    return e_x / e_x.sum(axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zwnsg9diFWCt"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    result = np.maximum(0, x) # ReLU activation\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PGyPLkST9V9c"
   },
   "outputs": [],
   "source": [
    "def predict(x,w1,w2,b1,b2 ):\n",
    "  #Initializing a variable to hold the value of the weighted sum (of input features and the corresponding weights) \n",
    "  weighted_1_sum=np.zeros((x.shape[0],w1.shape[1])) \n",
    "  weighted_2_sum=np.zeros((150,3)) \n",
    "  \n",
    "  ##print(\"instance in predict(p1,p2):\",instance)\n",
    "  ##print(\"weights in predict(p1,p2):\",weights)\n",
    "  \n",
    "  #calculating the weighted sum of the instance features and the corresponding weights as a dot product\n",
    "  weighted_1_sum=np.dot(x,w1)+b1\n",
    "  \n",
    "  ##print(\"weighted_sum in predict(p1,p2):\",weighted_sum)\n",
    "  \n",
    "  #Step activation function - maps non-negative inputs (function inputs, the weighted sum here) to 1 and negative inputs to 0. \n",
    "  #Using an if statement for the step activation function\n",
    "  activation_1 = relu(weighted_1_sum)\n",
    "  \n",
    "  #activation_1.insert(0,\"bias\", np.ones(X.shape[0]), True)\n",
    "  # activation_1 = np.insert(activation_1,0,1,axis = 1)\n",
    "\n",
    "  weighted_2_sum=np.dot(activation_1,w2)+b2\n",
    "  ##print (\"softmax of weighted sum =\",a)\n",
    "\n",
    "  activation_2=softmax(weighted_2_sum)\n",
    "  return weighted_2_sum,activation_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "AbIJOyfy8Yqs",
    "outputId": "122a5bf9-04fb-484e-8f2a-0e723c5a45ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([[1,2,3]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9r90cJeDlDR"
   },
   "source": [
    "## STEP 3: Define the function that trains the weight using backpropogation using gradient descent. \n",
    "\n",
    "Here, we use stochastic gradient descent which means we we go through all the instances one by one and update the weights after each time an instance is passed in the forward direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YdyxTUNCOT0z"
   },
   "source": [
    "**Weight update:**\n",
    "\n",
    "updated weight = old weight - update in weight\n",
    "\n",
    "> where:\n",
    "\n",
    "> update in the weight = learning rate * (desired output - actual (or predicted) output) * input\n",
    "  \n",
    "Each of the weights (corresponding to each input feature) gets updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIdxZB-bIV_h"
   },
   "outputs": [],
   "source": [
    "# from math import log\n",
    " \n",
    "# # calculate categorical cross entropy\n",
    "# def categorical_cross_entropy(target, predicted):\n",
    "#   sum_score = 0.0\n",
    "#   print(target)\n",
    "#   print(len(target))\n",
    "#   for j in range(len(target)):\n",
    "#     sum_score += target[j] * log(1e-15 + predicted[j])\n",
    "# \treturn sum_score\n",
    "#   #https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzVIgQscQkqo"
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "def categorical_cross_entropy(target, predicted):\n",
    "  sum_score = 0.0\n",
    "  \n",
    "  ##print(\"predicted in CCE=\",predicted)\n",
    "  ##print(predicted[0])\n",
    "  \n",
    "  for j in range(len(target)):\n",
    "    sum_score += target.iloc[j] * log(1e-15 + predicted[j])\n",
    "  return sum_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "TdVDQIkysKlu",
    "outputId": "b7f235ff-d30d-4701-f917-dee2f510d0bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.ones((X.shape[1], y.nunique())).shape\n",
    "y_total=y.to_numpy()\n",
    "len(np.unique(y_total,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqxRPYqri1LB"
   },
   "outputs": [],
   "source": [
    "#learning_rate and epochs are hyperparameters here, that is, we specify their values (we try different values to see which one helps our model train / converge the best)\n",
    "def train_weights(x,y, learning_rate, epochs):\n",
    "  \n",
    "  #initializing a weight vector (array of 1 dimension here as there is only one neuron) whose length is equal to the number of input features\n",
    "  w1 = np.random.randn(4,10) #The no. of input features in x is given by the number of columns x has. It can be obtained from the shape. Run x.shape and see what you get.\n",
    "  b1=np.zeros((1,10))\n",
    "  w2 = np.random.randn(10,3) #The no. of input features in x is given by the number of columns x has. It can be obtained from the shape. Run x.shape and see what you get.\n",
    "  b2=np.zeros((1,3))\n",
    "\n",
    "  #We look at the error in predictions for all instances, and do that for each epoch\n",
    "  for epoch in range(epochs): #for every epoch\n",
    "    total_error = 0.0 #storing the total error for that epoch\n",
    "    prediction_softmax,activation_hidden = predict(x, w1, w2, b1, b2)\n",
    "    prediction=np.argmax(prediction_softmax,axis=0)\n",
    "      \n",
    "      ##print(\"shape of prediction_softmax and of prediction=\",prediction_softmax.shape,prediction.shape)\n",
    "      ##print(\"Prediction softmax in train_weights:\",prediction_softmax)\n",
    "    \n",
    "    error_2 = y-prediction_softmax\n",
    "      ##print(\"error shape=\",error.shape)\n",
    "      ##print(\"type(error)=\",type(error))\n",
    "     #sum of squared error\n",
    "    error_2/=150\n",
    "    \n",
    "    w2_update = np.dot(activation_hidden.T, error_2)\n",
    "    #b2_update = np.sum(error_2, axis=0, keepdims=True)\n",
    "    b2_update = np.sum(error_2, axis=0).values.reshape(1,3)\n",
    "    print(b2_update)  \n",
    "        # next backprop into hidden layer\n",
    "    error_1 = np.dot(error_2, w2.T)\n",
    "        # backprop the ReLU non-linearity\n",
    "    error_1[activation_hidden <= 0] = 0\n",
    "        # finally into W,b\n",
    "    w1_update = np.dot(x.T, error_1)\n",
    "    b1_update = np.sum(error_1, axis=0)\n",
    "    \n",
    "    w1=w1+learning_rate*w1_update\n",
    "    b1=b1+learning_rate*b1_update\n",
    "    w2=w2+learning_rate*w2_update\n",
    "    print(\"w2 shape\",w2.shape)\n",
    "    print(\"learning rate*w2 update\",(learning_rate*w2_update).shape)\n",
    "    print(\"b2 shape\",b2.shape)\n",
    "    print(\"learning rate*b2 update\",(learning_rate*b2_update).shape)\n",
    "    b2=b2+learning_rate*b2_update\n",
    "    ##print(weights)\n",
    "    \n",
    "    #printing the epoch number, the learning rate and mean of the total error for that epoch\n",
    "    print(f'Epochs = {epoch}, learning rate = {learning_rate}') #mean categorical cross entropy error = {mean_error}'\n",
    "  return w1,w2,b1,b2 #these are the final, trained weights after running through all the epochs - used in prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-1OHhycWfLaE"
   },
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entrop\n",
    "# ∂E∂wij=yi(oj−tj)y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cq6vIyE_EDy4"
   },
   "source": [
    "## STEP 4: Define the function that serves as the model \n",
    "\n",
    "This function will call the training and predict functions, and computes the final predicted outcome after running through all the epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2TcMOiRMKjL3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gN8iRg0UKb8Y"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,a_train,a_test=train_test_split(X, y, test_size=0.2, random_state=2275)\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0GkX0XhNgtT"
   },
   "outputs": [],
   "source": [
    "y_test=np.argmax(np.array(a_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QfJHFMbcneGZ"
   },
   "outputs": [],
   "source": [
    "#defining the function that calls the training function, and computes the final outcome\n",
    "def perceptron(x_train, y_train,x_test, learning_rate, epochs):\n",
    "  #y_preds = list() #initializing a variable to hold the predicted values (one prediction for each instance) as a list\n",
    "  w1,w2,b1,b2 = train_weights(x_train, y_train, learning_rate, epochs)\n",
    " # for i in range(len(x_test)):\n",
    "    #for each instance/ row of X, getting the prediction and appending it to the list\n",
    "  prediction_softmax,activation_hidden = predict(x_test,w1,w2,b1,b2)\n",
    "  print(\"prediction_softmax in perceptron()\",prediction_softmax)\n",
    "  prediction=np.argmax(prediction_softmax,axis=1)\n",
    "  print(\"prediction in perceptron()\",prediction)\n",
    "  return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdxwqjbnEuoC"
   },
   "source": [
    "## STEP 5: Define a function that evaluates the accuracy score\n",
    "\n",
    "Now that we have predicted the outcome, we need to compute the accuracy of our model. We use a simple accuracy metric in this code, as described below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVFa-voKvOQn"
   },
   "outputs": [],
   "source": [
    "def accuracy_score(y_desired, y_predicted):\n",
    "  correct = 0\n",
    "  \n",
    "  #computing accuracy as the percent correctness of the predictions among the instances\n",
    "  #Defined here as the %percent of correct predictions out of all the instances the model predicted for\n",
    "  print(\"predicted:\",y_predicted)\n",
    "  print(\"desired:\",y_desired)\n",
    "  for i in range(len(y_desired)):\n",
    "    if y_desired[i] == y_predicted[i]:\n",
    "      correct += 1\n",
    "  accuracy_score=correct*100.0/len(y_desired)\n",
    "  return accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5p7k612MpTDI"
   },
   "source": [
    "### STEP 6: Tune the hyperparameters, run the model, obtain the accuracy score. Repeat for different values of the hyperparameters and see what values give you the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "eBmoEK8T0huB",
    "outputId": "1e7b30b1-f4d6-421a-e583-618866e7dc73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.36906705 -0.5866628  -5.698111  ]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 0, learning rate = 0.01\n",
      "[[-18.17845065 -11.29176809  14.34521235]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 1, learning rate = 0.01\n",
      "[[0.34258893 0.48057701 0.16597085]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 2, learning rate = 0.01\n",
      "[[0.34241228 0.43672297 0.17432494]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 3, learning rate = 0.01\n",
      "[[0.34070898 0.41073895 0.18061817]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 4, learning rate = 0.01\n",
      "[[0.33866499 0.3924056  0.18440102]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 5, learning rate = 0.01\n",
      "[[0.33634669 0.38077021 0.1858494 ]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 6, learning rate = 0.01\n",
      "[[0.33384392 0.37232216 0.18655548]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 7, learning rate = 0.01\n",
      "[[0.33126386 0.36565002 0.18677128]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 8, learning rate = 0.01\n",
      "[[0.32866785 0.36035681 0.1863948 ]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 9, learning rate = 0.01\n",
      "[[0.32609259 0.35573972 0.18564323]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 10, learning rate = 0.01\n",
      "[[0.32352568 0.35148523 0.18477062]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 11, learning rate = 0.01\n",
      "[[0.32095453 0.34753315 0.18388069]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 12, learning rate = 0.01\n",
      "[[0.31839189 0.34399785 0.18283349]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 13, learning rate = 0.01\n",
      "[[0.31584676 0.34061413 0.18173749]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 14, learning rate = 0.01\n",
      "[[0.313322   0.33728511 0.18063398]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 15, learning rate = 0.01\n",
      "[[0.31081743 0.33400926 0.17952369]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 16, learning rate = 0.01\n",
      "[[0.30833486 0.33083489 0.17836655]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 17, learning rate = 0.01\n",
      "[[0.30587373 0.32775112 0.17717099]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 18, learning rate = 0.01\n",
      "[[0.303432   0.32470929 0.17597638]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 19, learning rate = 0.01\n",
      "[[0.30100951 0.32170849 0.174783  ]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 20, learning rate = 0.01\n",
      "[[0.29860613 0.31874781 0.17359115]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 21, learning rate = 0.01\n",
      "[[0.29622257 0.31584818 0.17238321]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 22, learning rate = 0.01\n",
      "[[0.29385675 0.31305246 0.17114374]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 23, learning rate = 0.01\n",
      "[[0.29150652 0.310314   0.16991158]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 24, learning rate = 0.01\n",
      "[[0.28917507 0.30760505 0.16868494]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 25, learning rate = 0.01\n",
      "[[0.28686223 0.30492515 0.16746391]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 26, learning rate = 0.01\n",
      "[[0.28456786 0.30227388 0.16624857]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 27, learning rate = 0.01\n",
      "[[0.28229182 0.2996508  0.16503899]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 28, learning rate = 0.01\n",
      "[[0.28003394 0.2970555  0.16383523]]\n",
      "w2 shape (10, 3)\n",
      "learning rate*w2 update (10, 3)\n",
      "b2 shape (1, 3)\n",
      "learning rate*b2 update (1, 3)\n",
      "Epochs = 29, learning rate = 0.01\n",
      "prediction_softmax in perceptron() [[-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.02652626 -0.0556939   0.13370748]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.00117591 -0.26363124  0.11948665]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]\n",
      " [-0.03064162 -0.02193746  0.13601609]]\n",
      "prediction in perceptron() [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "predicted\n",
      "predicted: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "desired: [0 2 1 0 1 0 1 2 2 1 0 2 0 2 0 2 1 0 0 1 0 1 1 0 0 1 2 2 0 2]\n",
      "\n",
      "\n",
      "The accuracy is 30.0%.\n"
     ]
    }
   ],
   "source": [
    "#Iitializing the hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 30\n",
    "\n",
    "#calling the model, predicting the outcome and measuring the accuracy of the prediction\n",
    "predicted = perceptron(X_train, a_train, X_test, learning_rate, epochs)\n",
    "\n",
    "print(\"predicted\")\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"The accuracy is {accuracy}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dix7EQ4p_76O"
   },
   "outputs": [],
   "source": [
    "final=pd.DataFrame(y_test,columns=[\"y_test\"])\n",
    "final[\"y_pred\"]=predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "USqNRFdDBGsN",
    "outputId": "02cce0bb-8e6c-451a-b510-972401623a4c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-98edc3421e0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"x1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"x2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\">\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#plt.plot(data[data[\"y\"]==1][\"x1\"],data[data[\"y\"]==1][\"x2\"],'o',c=\"b\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xlim(-1,3)\n",
    "plt.xticks(np.arange(0,3,1))\n",
    "plt.yticks(np.arange(0,3,1))\n",
    "plt.ylim(-1,3)\n",
    "plt.scatter(X[data[\"y\"]==1][\"x1\"], X[data[\"y\"]==1][\"x2\"], s=80, c=\"r\", marker=\">\")\n",
    "\n",
    "#plt.plot(data[data[\"y\"]==1][\"x1\"],data[data[\"y\"]==1][\"x2\"],'o',c=\"b\")\n",
    "plt.plot(data[data[\"y\"]==0][\"x1\"],data[data[\"y\"]==0][\"x2\"],'^',c=\"r\")\n",
    "plt.title('Graph')\n",
    "plt.xlabel('x', color='#1C2833')\n",
    "plt.ylabel('y', color='#1C2833')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BgqKWb4W8xRc"
   },
   "outputs": [],
   "source": [
    " # plt.subplot(321)\n",
    "# plt.scatter(X[data[\"y\"]==1][\"x1\"], X[data[\"y\"]==1][\"x2\"], s=80, c=\"r\", marker=\">\")\n",
    "# for i in range(len(data)):\n",
    "#   x1=data.iloc[i,0]\n",
    "#   x2=data.iloc[i:1]\n",
    "#   plt.text(x1,x2,f\"{x1},{x2}\")\n",
    "#   print(\"check\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DX2P_MultiLayer_Perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
